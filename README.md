# Hugging-face
---

### 📚 Best Book:

* **"Natural Language Processing with Transformers"** by *Lewis Tunstall, Leandro von Werra, and Thomas Wolf* (Thomas Wolf is a Hugging Face cofounder!)

  * [Official book site](https://transformersbook.com/)
  * Covers Hugging Face `transformers`, `datasets`, and `accelerate` libraries very well.
  * Hands-on tutorials + real-world projects (text classification, question answering, etc.)
  * It's *officially endorsed* by Hugging Face.

---

### 🛠️ Best GitHub Repositories:

1. **Hugging Face Course**
   → [https://github.com/huggingface/course](https://github.com/huggingface/course)

   * This is the *official Hugging Face curriculum*.
   * Free, project-driven, teaches `transformers`, `datasets`, `tokenizers`, and fine-tuning.
   * Best if you want a structured step-by-step guide.

2. **Hugging Face Transformers (codebase itself)**
   → [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)

   * Open the `examples/` folder inside — it has examples for text, vision, audio, and multimodal tasks.
   * Best if you want to learn by **reverse engineering real examples**.

3. **Hugging Face NLP Course Notebooks by Dair AI**
   → [https://github.com/dair-ai/huggingface-course](https://github.com/dair-ai/huggingface-course)

   * Beautifully cleaned Jupyter notebooks for each Hugging Face course chapter.
   * Helpful if you want to learn directly inside Colab or Jupyter.

4. **Hugging Face Diffusers (for generative AI)**
   → [https://github.com/huggingface/diffusers](https://github.com/huggingface/diffusers)

   * If you're aiming for *text-to-image* generation models (like Stable Diffusion).
   * Covers pipelines, schedulers, models related to generative diffusion models.

---

### ✨ My Recommendation to Start:

* **First**, do the [Hugging Face Course GitHub](https://github.com/huggingface/course).
* **Then**, get the **"Natural Language Processing with Transformers"** book alongside it.
* **Finally**, explore the `examples/` folder in the [huggingface/transformers GitHub](https://github.com/huggingface/transformers) when you're ready for custom/advanced experiments.

---

Here are some of the **best free and high-quality tutorials** to learn **Transformers** (specifically for NLP, LLMs, and generative AI):

---

### 🔥 1. **Hugging Face Transformers Course (Official)**

**📍 URL**: [https://huggingface.co/learn/nlp-course](https://huggingface.co/learn/nlp-course)
**🧠 Best for**: Beginners to intermediate
**✅ Features**:

* Covers the basics of tokenizers, models, pipelines
* Includes hands-on with `transformers` library
* Teaches how to use and fine-tune BERT, GPT2, etc.
* Integrated with Google Colab notebooks
* Constantly updated by Hugging Face

> 🎓 **Most recommended starting point**.

---

### 🔍 2. **The Illustrated Transformer by Jay Alammar**

**📍 URL**: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)
**🧠 Best for**: Conceptual and visual understanding
**✅ Features**:

* Step-by-step visual breakdown of the Transformer architecture
* Explains self-attention with diagrams
* Highly beginner-friendly for the theory side

> 🔍 A must-read before diving into code.

---

### 💻 3. **FastAI + Hugging Face Integration Tutorial**

**📍 URL**: [https://huggingface.co/blog/fastai-transformers](https://huggingface.co/blog/fastai-transformers)
**🧠 Best for**: FastAI users or learners who want practical fine-tuning
**✅ Features**:

* Shows how to use Hugging Face with the fastai library
* Focuses on text classification using Transformers
* Great for rapid experimentation

---

### 📘 4. **Google’s Transformer Architecture Paper Summary**

**📍 URL**: [https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)
**🧠 Best for**: Original paper and official summary
**✅ Features**:

* Direct explanation of “Attention is All You Need”
* Links to official research

---

### 🛠️ 5. **Practical Transformer (Colab Notebook by Lilian Weng)**

**📍 URL**: [https://lilianweng.github.io/posts/2018-06-24-attention/](https://lilianweng.github.io/posts/2018-06-24-attention/)
**🧠 Best for**: Intermediate to advanced
**✅ Features**:

* Breaks down attention mechanism in code
* Great for those who want math + code hybrid understanding

---

### ✅ Bonus: GitHub Repos for Practice

* **`transformers` GitHub**: [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)
* Contains tons of examples: training, inference, generation, summarization

---


