## ğŸ”§ **LoRA / PEFT (Parameter-Efficient Fine-Tuning)**

---

### âœ… What is PEFT?

**Parameter-Efficient Fine-Tuning (PEFT)** refers to a set of techniques that **fine-tune only a small part** of a large model instead of the entire parameter set. Itâ€™s ideal for:

* Large models (BLOOM, LLaMA, Falcon, etc.)
* Resource-constrained environments (low GPU RAM)
* Domain-specific or task-specific adaptation

---

### âœ… What is LoRA?

**LoRA (Low-Rank Adaptation)** is the most popular PEFT method. Instead of fine-tuning the full weight matrices (millions/billions of parameters), LoRA **adds small trainable matrices (rank `r`)** to selected layers and **freezes the base model**.

It introduces **two low-rank matrices (A and B)** such that:

$$
W' = W + \Delta W \quad \text{where} \quad \Delta W = A \cdot B
$$

---

### âœ… Benefits of LoRA

| Feature                   | Benefit                                                     |
| ------------------------- | ----------------------------------------------------------- |
| ğŸ§  Fewer Trainable Params | Only a few million parameters                               |
| ğŸš€ Faster Training        | Smaller gradient updates                                    |
| ğŸ§Š Lower Memory           | Doesn't update the entire model                             |
| ğŸ”„ Reusable               | Can switch tasks quickly by loading different LoRA adapters |
| ğŸŒ Community-ready        | Integrated with Hugging Face `peft` library                 |


---

### ğŸ”§ Key Hyperparameters in LoRA

| Hyperparameter   | Description                                               | Example                |
| ---------------- | --------------------------------------------------------- | ---------------------- |
| `r`              | Rank of the low-rank update matrices A and B              | `8`, `16`              |
| `lora_alpha`     | Scaling factor for the low-rank matrices                  | `16`, `32`             |
| `lora_dropout`   | Dropout probability for LoRA layers                       | `0.05`, `0.1`          |
| `bias`           | Whether to train bias params (`none`, `all`, `lora_only`) | `"none"`               |
| `target_modules` | Model modules to apply LoRA (e.g., attention projections) | `["q_proj", "v_proj"]` |
| `task_type`      | Defines type of task (`CAUSAL_LM`, `SEQ_CLS`, etc.)       | `TaskType.CAUSAL_LM`   |

---

### ğŸ“¦ Hugging Face `peft` Library Tasks

| Task Type            | Use Case                           |
| -------------------- | ---------------------------------- |
| `CAUSAL_LM`          | Text generation (e.g., GPT, LLaMA) |
| `SEQ_CLS`            | Sequence classification            |
| `TOKEN_CLS`          | NER or token tagging               |
| `SEQ_2_SEQ_LM`       | Summarization, translation         |
| `QUESTION_ANS`       | Extractive QA                      |
| `FEATURE_EXTRACTION` | Embedding extraction               |

---

### ğŸ§  Under the Hood

LoRA modifies **only certain weights** like `query`, `key`, `value` in attention layers. Instead of updating the full 7B+ parameters, LoRA might only update \~5M parameters (a 1000x reduction in training size).

---

### ğŸ“š Example Use Cases

* Fine-tune **LLaMA2** on **medical Q\&A** with LoRA and just 1 GPU.
* Use LoRA for **multi-lingual adaptation** of a GPT model.
* Plug in LoRA adapters into an **8-bit quantized model** (via `bitsandbytes`).

---

### ğŸ“ Tools that Support LoRA

| Tool              | Role                               |
| ----------------- | ---------------------------------- |
| ğŸ¤— `peft`         | Core PEFT library                  |
| ğŸ¤— `transformers` | Model loading and integration      |
| `bitsandbytes`    | 8-bit / 4-bit quantization         |
| `accelerate`      | Multi-GPU and optimization support |
| `wandb`           | Logging and visualization          |

---

### ğŸ“Œ Best Practices

* Use `load_in_8bit=True` to conserve GPU RAM.
* Combine LoRA with **gradient checkpointing** for large models.
* Tune `r` and `lora_alpha` for balance between speed and accuracy.

---

---


---

Hereâ€™s a **concise yet detailed breakdown** of the top **parameter-efficient fine-tuning (PEFT)** methods used in Large Language Models (LLMs), including:

> **PEFT Â· LoRA Â· QLoRA Â· Adapters Â· Prompt Tuning**

These are **complementary**, not mutually exclusiveâ€”often used based on compute budget, task size, or deployment needs.

---

## ğŸ§  1. **PEFT (Parameter-Efficient Fine-Tuning)** â€“ *The umbrella concept*

### ğŸ”¹ Definition:

A collection of techniques that **fine-tune only a small subset of model parameters** (or add a few trainable ones) instead of updating the full model.

### ğŸ”¹ Why PEFT?

* Reduce compute and memory requirements
* Faster training on consumer-grade hardware (1 GPU or even CPU)
* Easier to store/share adapters (\~MBs vs GBs)

### ğŸ”¹ Types:

* LoRA
* QLoRA
* Adapters
* Prefix/Prompt tuning
* BitFit, IA3, etc.

---

## ğŸ”§ 2. **LoRA (Low-Rank Adaptation)**

### ğŸ”¹ Principle:

Instead of updating large matrices, **add two small trainable matrices (A & B)** of rank `r` to certain layers:

$$
\Delta W = A \cdot B
$$

### ğŸ”¹ Characteristics:

| Feature               | Value                         |
| --------------------- | ----------------------------- |
| Trainable Params      | \~0.1â€“2%                      |
| Memory Efficient      | âœ…                             |
| Use with 8-bit models | âœ…                             |
| Popular With          | Causal LM, Seq2Seq, NER, etc. |
| HF Tool               | `peft.LoraConfig`             |

### ğŸ”¹ Hyperparameters:

* `r`: Rank of LoRA (e.g. `8`, `16`)
* `lora_alpha`: Scaling factor
* `lora_dropout`: Dropout on LoRA layers
* `target_modules`: Layers to apply (e.g. `q_proj`, `v_proj`)

---

## âš¡ 3. **QLoRA (Quantized LoRA)**

### ğŸ”¹ Principle:

Combines **4-bit quantized models** with LoRA adapters.

$$
\text{Quantize} \rightarrow \text{LoRA fine-tune only adapters}
$$

### ğŸ”¹ Benefits:

| Feature     | Value                                  |
| ----------- | -------------------------------------- |
| Model Size  | 4x smaller (e.g., 65B â†’ 16GB)          |
| RAM Usage   | Fits on a single 24GB GPU              |
| Performance | Comparable to full finetuning          |
| Tools       | `bitsandbytes`, `peft`, `transformers` |

### ğŸ”¹ Typical Stack:

* Load model with `load_in_4bit=True` (via `bitsandbytes`)
* Use `bnb_4bit_compute_dtype='bfloat16'`
* Add `LoRA` via `peft`

---

## âš™ï¸ 4. **Adapters**

### ğŸ”¹ Principle:

Inject **small bottleneck layers (adapter blocks)** inside the transformer layers, and fine-tune **only the adapters**.

### ğŸ”¹ Anatomy of Adapter:

```
LayerNorm â†’ DownProj â†’ Nonlinearity â†’ UpProj â†’ Residual Add
```

### ğŸ”¹ Characteristics:

| Feature                  | Value                                         |
| ------------------------ | --------------------------------------------- |
| Trainable Params         | \~1â€“2%                                        |
| Modular Plug & Play      | âœ…                                             |
| Can be used sequentially | âœ…                                             |
| Tasks                    | Classification, QA, NER, generation           |
| HF Tool                  | `adapter-transformers` (separate from `peft`) |

---

## âœï¸ 5. **Prompt Tuning / Prefix Tuning**

### ğŸ”¹ Prompt Tuning:

* Learnable **token embeddings** prepended to the input
* Rest of the model remains **frozen**
* Smallest number of trainable params

### ğŸ”¹ Prefix Tuning:

* Learns **prefix vectors per attention layer**
* Injects soft prompts at each layer, not just input

| Method        | Params     | Trainable             | Quality |
| ------------- | ---------- | --------------------- | ------- |
| Prompt Tuning | \~few KB   | Embeddings only       | Medium  |
| Prefix Tuning | \~0.1â€“0.5% | Attention keys/values | High    |
| LoRA          | \~0.5â€“2%   | Q/V layers or FFN     | Higher  |

### ğŸ”¹ Ideal For:

* Training with **very small data**
* Few-shot or zero-shot prompt augmentation
* When **storage size matters**

---

## ğŸ“Š Summary Table

| Method            | % Params Tuned | Uses Base Model | Needs Quantization | Ideal For                             |
| ----------------- | -------------- | --------------- | ------------------ | ------------------------------------- |
| **LoRA**          | 0.5% â€“ 2%      | âœ…               | Optional           | Most common PEFT                      |
| **QLoRA**         | 0.5% â€“ 2%      | 4-bit model     | âœ…                  | Very large models (e.g., 33B, 65B)    |
| **Adapters**      | \~2%           | âœ…               | âŒ (or optional)    | Modular tasks, multi-domain           |
| **Prompt Tuning** | <0.1%          | âœ…               | âŒ                  | Extremely low-resource fine-tuning    |
| **Prefix Tuning** | 0.1â€“0.5%       | âœ…               | âŒ                  | Better performance than prompt tuning |

---

## ğŸ§° Tools Used Across Methods

| Tool                   | Description                                            |
| ---------------------- | ------------------------------------------------------ |
| ğŸ¤— `peft`              | Official Hugging Face library for LoRA, Prefix, Prompt |
| `transformers`         | Base model loading and Trainer                         |
| `bitsandbytes`         | Quantization (for QLoRA)                               |
| `adapter-transformers` | Separate lib for Adapter-based fine-tuning             |
| `accelerate`           | Easy multi-GPU & mixed precision setup                 |

---

